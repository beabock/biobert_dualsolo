# Figure Captions for Manuscript

## Main Figures

**Figure 1.** Comparative model performance across four transformer-based architectures. Bar charts show mean classification metrics (accuracy, precision, recall, F1-score) ± standard deviation from stratified 5-fold cross-validation (n=56 abstracts). BioBERT (biomedical domain-adapted) and BERT-base-cased achieved statistically equivalent performance (~89% accuracy), substantially outperforming BERT-base-uncased (~75%) and BiodivBERT (~77%). Case sensitivity proved critical, with cased models outperforming uncased by ~15 percentage points. Metrics are calculated as macro averages (unweighted mean across dual and solo classes). Source: [figures/model_comparison.png](../figures/model_comparison.png)

**Figure 2.** Aggregated confusion matrices for all four models (BioBERT, BERT-base-cased, BERT-base-uncased, BiodivBERT) across all 5 folds (total 56 predictions per model). Each matrix shows true labels (Solo = single trophic mode, Dual = multiple trophic modes) versus predicted labels, allowing direct comparison of error patterns and class balance for each model. BioBERT and BERT-base-cased show balanced performance, while uncased and BiodivBERT models display more misclassifications. Color intensity indicates prediction frequency; diagonal cells represent correct predictions. Source: [figures/confusion_matrices_comparison.png](../figures/confusion_matrices_comparison.png)

**Figure 3.** Training time comparison across models. BioBERT and BERT-base-cased completed 5-fold cross-validation training in ~10-11 minutes, while BiodivBERT and BERT-base-uncased required ~35 minutes. Differences likely reflect tokenization efficiency and convergence patterns rather than model size (all models have ~110M parameters). Faster convergence in cased models correlates with higher classification accuracy, suggesting that case-preserving tokenization provides stronger learning signals for this taxonomic text classification task. Training performed on NAU Monsoon HPC cluster (Tesla K80 GPU, CUDA 11.4). Source: [figures/timing_comparison.png](../figures/timing_comparison.png)

**Figure 4.** Workflow diagram of the classification pipeline. The diagram summarizes the full end-to-end process: literature search (Web of Science queries), manual curation (56 labeled abstracts), preprocessing (text cleaning, tokenization with truncation at 512 tokens and token-length QC), stratified 5-fold cross-validation, model fine-tuning across four models (BERT-base-uncased, BERT-base-cased, BioBERT v1.1, BiodivBERT) with standardized hyperparameters, evaluation (metrics, confusion matrices, learning curves), and outputs (fine-tuned models, predictions). Source: `assets/pipeline_diagram.md` (markdown diagram source) and [`figures/pipeline_diagram.png`](../figures/pipeline_diagram.png).

## Supplementary Materials

**Supplementary Figure S1.** Token length distribution across all 56 abstracts. Most abstracts (53/56, 94.6%) fell within the 512-token limit of transformer models. Three abstracts (5.4%) exceeded this limit and were truncated, though truncation typically affects concluding sentences where classification-relevant information is less concentrated. Mean token count: 412 ± 89 tokens (range: 187-628). This analysis demonstrates that standard transformer sequence lengths are generally adequate for abstract-based classification tasks in fungal ecology. Source: [results/token_length_stats.json](../results/token_length_stats.json)

**Supplementary Table S1.** Detailed error analysis showing abstracts misclassified by at least one model. Seven abstracts were misclassified by 2+ models (flagged as particularly ambiguous cases). For each error: abstract text (truncated), true label, predicted label, model confidence scores, fold number, and total number of models that misclassified the sample. Common patterns include: (1) endophytes described with both plant-beneficial and decomposition roles, (2) fungi switching lifestyles based on host status, and (3) abstracts emphasizing ecological context over explicit trophic mode terminology. This analysis highlights where human curation remains essential and informs future annotation guideline development. Source: [figures/error_analysis.csv](../figures/error_analysis.csv)

---

## Usage Notes

- All figure files are available in the `figures/` directory
- High-resolution versions suitable for publication can be generated by re-running `generate_report_v2.py` with `dpi=300` in matplotlib settings
- Figure source code is available in `generate_report_v2.py` under the visualization functions
- Interactive HTML report with all figures: [project_report_comparison.html](../project_report_comparison.html)

## Figure File Paths

```
figures/
├── model_comparison.png          # Figure 1
├── confusion_matrices_comparison.png  # Figure 2
├── timing_comparison.png         # Figure 3
└── error_analysis.csv            # Supplementary Table S1

results/
├── token_length_stats.json       # Supplementary Figure S1 data
├── all_model_metrics.json        # Summary statistics
└── fold_predictions_*.csv        # Per-fold detailed predictions
```

## Citation

When using these figures, please cite:

Bock, B.M. (2025). Automated Extraction of Fungal Trophic Modes from Literature Using BioBERT: An Open Pilot Workflow. *RIO Journal*. [DOI and repository link to be added upon publication]
