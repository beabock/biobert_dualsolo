{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BioBERT Dual Lifestyle Classifier\n",
    "\n",
    "This notebook demonstrates the BioBERT classification pipeline for distinguishing between dual and solo lifestyles in biological abstracts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Loading and Preprocessing\n",
    "\n",
    "Load the training and test datasets, and show the train/test split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'nlp_bio (Python 3.9.23)' requires the ipykernel package.\n",
      "\u001b[1;31m<a href='command:jupyter.createPythonEnvAndSelectController'>Create a Python Environment</a> with the required packages.\n",
      "\u001b[1;31mOr install 'ipykernel' using the command: 'conda install -n nlp_bio ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load the datasets\n",
    "train_df = pd.read_csv('train.csv')\n",
    "test_df = pd.read_csv('test.csv')\n",
    "\n",
    "# Display basic info\n",
    "print('Training set shape:', train_df.shape)\n",
    "print('Test set shape:', test_df.shape)\n",
    "print('\\nTraining set class distribution:')\n",
    "print(train_df['label'].value_counts())\n",
    "print('\\nTest set class distribution:')\n",
    "print(test_df['label'].value_counts())\n",
    "\n",
    "# Show sample data\n",
    "print('\\nSample training data:')\n",
    "print(train_df.head())\n",
    "print('\\nSample test data:')\n",
    "print(test_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Model Fine-tuning with Training Curves\n",
    "\n",
    "Load the fine-tuned BioBERT model and display training curves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, Trainer\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the fine-tuned model and tokenizer\n",
    "model_path = 'fine_tuned_biobert_classification'\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_path)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "\n",
    "# Load training history from trainer state\n",
    "trainer_state_path = 'results/trainer_state.json'  # Using the latest checkpoint\n",
    "with open(trainer_state_path, 'r') as f:\n",
    "    trainer_state = json.load(f)\n",
    "\n",
    "# Extract training metrics\n",
    "log_history = trainer_state['log_history']\n",
    "\n",
    "# Plot training curves\n",
    "steps = [entry.get('step', 0) for entry in log_history if 'loss' in entry]\n",
    "train_loss = [entry['loss'] for entry in log_history if 'loss' in entry]\n",
    "eval_loss = [entry.get('eval_loss', None) for entry in log_history if 'eval_loss' in entry]\n",
    "eval_accuracy = [entry.get('eval_accuracy', None) for entry in log_history if 'eval_accuracy' in entry]\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.plot(steps, train_loss, label='Training Loss')\n",
    "plt.xlabel('Step')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "valid_steps = [entry.get('step', 0) for entry in log_history if 'eval_loss' in entry]\n",
    "plt.plot(valid_steps, eval_loss, label='Validation Loss')\n",
    "plt.xlabel('Step')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Validation Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.plot(valid_steps, eval_accuracy, label='Validation Accuracy')\n",
    "plt.xlabel('Step')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Validation Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Evaluation on Test Set\n",
    "\n",
    "Evaluate the model on the test set and compute metrics: accuracy, precision, recall, F1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "# Custom dataset class\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_len=512):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts[idx])\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=self.max_len,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "# Prepare test dataset\n",
    "test_texts = test_df['abstract'].tolist()\n",
    "test_labels = test_df['label'].tolist()\n",
    "test_dataset = TextDataset(test_texts, test_labels, tokenizer)\n",
    "\n",
    "# Create trainer for evaluation\n",
    "trainer = Trainer(model=model)\n",
    "\n",
    "# Get predictions\n",
    "predictions = trainer.predict(test_dataset)\n",
    "preds = torch.argmax(torch.tensor(predictions.predictions), axis=1).numpy()\n",
    "true_labels = test_labels\n",
    "\n",
    "# Calculate metrics\n",
    "accuracy = accuracy_score(true_labels, preds)\n",
    "precision = precision_score(true_labels, preds, average='binary')\n",
    "recall = recall_score(true_labels, preds, average='binary')\n",
    "f1 = f1_score(true_labels, preds, average='binary')\n",
    "\n",
    "print(f'Accuracy: {accuracy:.4f}')\n",
    "print(f'Precision: {precision:.4f}')\n",
    "print(f'Recall: {recall:.4f}')\n",
    "print(f'F1 Score: {f1:.4f}')\n",
    "print('\\nClassification Report:')\n",
    "print(classification_report(true_labels, preds, target_names=['Solo', 'Dual']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Example Predictions\n",
    "\n",
    "Make predictions on example abstracts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example abstracts\n",
    "examples = [\n",
    "    \"This fungus can switch between saprotrophic decomposition of dead organic matter and forming mutualistic symbioses with plant roots, demonstrating a dual trophic lifestyle.\",\n",
    "    \"This saprotrophic fungus specializes exclusively in decomposing dead plant material and cannot form symbiotic associations.\",\n",
    "    \"The pathogenic fungus infects living plants but can also survive saprotrophically on dead plant tissues, exhibiting dual trophic modes.\",\n",
    "    \"This obligate symbiont fungus can only survive in mutualistic association with its host plant and cannot decompose dead organic matter independently.\"\n",
    "]\n",
    "\n",
    "# Tokenize and predict\n",
    "model.eval()\n",
    "predictions = []\n",
    "for text in examples:\n",
    "    inputs = tokenizer(text, return_tensors='pt', truncation=True, padding=True, max_length=512)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        pred = torch.argmax(outputs.logits, dim=1).item()\n",
    "        predictions.append('Dual' if pred == 1 else 'Solo')\n",
    "\n",
    "# Display results\n",
    "for i, (text, pred) in enumerate(zip(examples, predictions)):\n",
    "    print(f'Example {i+1}: {pred}')\n",
    "    print(f'Text: {text}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_bio",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
